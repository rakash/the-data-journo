{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Life-Cycle in PyTorch\n",
    "\n",
    "- comments: true\n",
    "- author: Akash Ramkumar\n",
    "- categories: [Deep Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the PyTorch API.\n",
    "\n",
    "The five steps in the life-cycle are as follows:\n",
    "\n",
    "1. Prepare the Data.\n",
    "2. Define the Model.\n",
    "3. Train the Model.\n",
    "4. Evaluate the Model.\n",
    "5. Make Predictions.\n",
    "\n",
    "Lets do the same with MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks, or CNNs for short, are a type of network designed for image input.\n",
    "\n",
    "They are comprised of models with convolutional layers that extract features (called feature maps) and pooling layers that distill features down to the most salient elements.\n",
    "\n",
    "CNNs are best suited to image classification tasks, although they can be used on a wide array of tasks that take images as input.\n",
    "\n",
    "A popular image classification task is the MNIST handwritten digit classification. It involves tens of thousands of handwritten digits that must be classified as a number between 0 and 9.\n",
    "\n",
    "The torchvision API provides a convenience function to download and load this dataset directly.\n",
    "\n",
    "The example below loads the dataset and plots the first few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define location to save or load the dataset\n",
    "path = './Pytorch Workflow with MNIST'\n",
    "# define the transforms to apply to the data\n",
    "trans = Compose([ToTensor()])\n",
    "\n",
    "# download and define the datasets\n",
    "train = MNIST(path, train=True, download=True, transform=trans)\n",
    "test = MNIST(path, train=False, download=True, transform=trans)\n",
    "\n",
    "# define how to enumerate the datasets\n",
    "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=32, shuffle=True)\n",
    "\n",
    "# get one batch of images\n",
    "\n",
    "i, (inputs, targets) = next(enumerate(train_dl))\n",
    "# plot some images\n",
    "for i in range(25):\n",
    "    # define subplot\n",
    "    pyplot.subplot(5, 5, i+1)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(inputs[i][0], cmap='gray')\n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a CNN model to classify the images in the MNIST dataset.\n",
    "\n",
    "Note that the images are arrays of grayscale pixel data, therefore, we must add a channel dimension to the data before we can use the images as input to the model.\n",
    "\n",
    "It is a good idea to scale the pixel values from the default range of 0-255 to have a zero mean and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# prepare the dataset\n",
    "\n",
    "def prepare_data(path):\n",
    "    # define standardization    \n",
    "    trans = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    # load dataset\n",
    "    train = MNIST(path, train=True, download=True, transform=trans)\n",
    "    test = MNIST(path, train=False, download=True, transform=trans)\n",
    "    \n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=64, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Conv2d(n_channels, 32, (3,3))\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # first pooling layer\n",
    "        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Conv2d(32, 32, (3,3))\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # second pooling layer\n",
    "        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n",
    "        # fully connected layer\n",
    "        self.hidden3 = Linear(5*5*32, 100)\n",
    "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "        self.act3 = ReLU()\n",
    "        # output layer\n",
    "        self.hidden4 = Linear(100, 10)\n",
    "        xavier_uniform_(self.hidden4.weight)\n",
    "        self.act4 = Softmax(dim=1)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.pool1(X)\n",
    "        # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.pool2(X)\n",
    "        # flatten\n",
    "        X = X.view(-1, 4*4*50)\n",
    "        # third hidden layer\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        \n",
    "        # output layer\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(10):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        # convert to class labels\n",
    "        yhat = argmax(yhat, axis=1)\n",
    "        # reshape for stacking\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the above functions (Making Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the data\n",
    "\n",
    "path = './Pytorch Workflow with MNIST'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network\n",
    "model = CNN(1)\n",
    "\n",
    "## train the model\n",
    "train_model(train_dl, model)\n",
    "\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
