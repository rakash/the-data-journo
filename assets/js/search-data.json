{
  
    
        "post0": {
            "title": "Deep Learning Model Life-Cycle in PyTorch",
            "content": "Steps . A model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the PyTorch API. . The five steps in the life-cycle are as follows: . Prepare the Data. | Define the Model. | Train the Model. | Evaluate the Model. | Make Predictions. | Lets do the same with MNIST dataset . Convolutional Neural Networks, or CNNs for short, are a type of network designed for image input. . They are comprised of models with convolutional layers that extract features (called feature maps) and pooling layers that distill features down to the most salient elements. . CNNs are best suited to image classification tasks, although they can be used on a wide array of tasks that take images as input. . A popular image classification task is the MNIST handwritten digit classification. It involves tens of thousands of handwritten digits that must be classified as a number between 0 and 9. . The torchvision API provides a convenience function to download and load this dataset directly. . The example below loads the dataset and plots the first few images. . from torch.utils.data import DataLoader from torchvision.datasets import MNIST from torchvision.transforms import Compose from torchvision.transforms import ToTensor from matplotlib import pyplot # define location to save or load the dataset path = &#39;./Pytorch Workflow with MNIST&#39; # define the transforms to apply to the data trans = Compose([ToTensor()]) # download and define the datasets train = MNIST(path, train=True, download=True, transform=trans) test = MNIST(path, train=False, download=True, transform=trans) # define how to enumerate the datasets train_dl = DataLoader(train, batch_size=32, shuffle=True) test_dl = DataLoader(test, batch_size=32, shuffle=True) # get one batch of images i, (inputs, targets) = next(enumerate(train_dl)) # plot some images for i in range(25): # define subplot pyplot.subplot(5, 5, i+1) # plot raw pixel data pyplot.imshow(inputs[i][0], cmap=&#39;gray&#39;) # show the figure pyplot.show() . We can train a CNN model to classify the images in the MNIST dataset. . Note that the images are arrays of grayscale pixel data, therefore, we must add a channel dimension to the data before we can use the images as input to the model. . It is a good idea to scale the pixel values from the default range of 0-255 to have a zero mean and a standard deviation of 1. . Prepare the data . from numpy import vstack from numpy import argmax from pandas import read_csv from sklearn.metrics import accuracy_score from torchvision.datasets import MNIST from torchvision.transforms import Compose from torchvision.transforms import ToTensor from torchvision.transforms import Normalize from torch.utils.data import DataLoader from torch.nn import Conv2d from torch.nn import MaxPool2d from torch.nn import Linear from torch.nn import ReLU from torch.nn import Softmax from torch.nn import Module from torch.optim import SGD from torch.nn import CrossEntropyLoss from torch.nn.init import kaiming_uniform_ from torch.nn.init import xavier_uniform_ # prepare the dataset def prepare_data(path): # define standardization trans = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))]) # load dataset train = MNIST(path, train=True, download=True, transform=trans) test = MNIST(path, train=False, download=True, transform=trans) # prepare data loaders train_dl = DataLoader(train, batch_size=64, shuffle=True) test_dl = DataLoader(test, batch_size=1024, shuffle=False) return train_dl, test_dl . Defining the Model . class CNN(Module): # define model elements def __init__(self, n_channels): super(CNN, self).__init__() # input to first hidden layer self.hidden1 = Conv2d(n_channels, 32, (3,3)) kaiming_uniform_(self.hidden1.weight, nonlinearity=&#39;relu&#39;) self.act1 = ReLU() # first pooling layer self.pool1 = MaxPool2d((2,2), stride=(2,2)) # second hidden layer self.hidden2 = Conv2d(32, 32, (3,3)) kaiming_uniform_(self.hidden2.weight, nonlinearity=&#39;relu&#39;) self.act2 = ReLU() # second pooling layer self.pool2 = MaxPool2d((2,2), stride=(2,2)) # fully connected layer self.hidden3 = Linear(5*5*32, 100) kaiming_uniform_(self.hidden3.weight, nonlinearity=&#39;relu&#39;) self.act3 = ReLU() # output layer self.hidden4 = Linear(100, 10) xavier_uniform_(self.hidden4.weight) self.act4 = Softmax(dim=1) # forward propagate input def forward(self, X): # input to first hidden layer X = self.hidden1(X) X = self.act1(X) X = self.pool1(X) # second hidden layer X = self.hidden2(X) X = self.act2(X) X = self.pool2(X) # flatten X = X.view(-1, 4*4*50) # third hidden layer X = self.hidden3(X) X = self.act3(X) # output layer X = self.hidden4(X) X = self.act4(X) return X . Train the Model . def train_model(train_dl, model): # define the optimization criterion = CrossEntropyLoss() optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # enumerate epochs for epoch in range(10): # enumerate mini batches for i, (inputs, targets) in enumerate(train_dl): # clear the gradients optimizer.zero_grad() # compute the model output yhat = model(inputs) # calculate loss loss = criterion(yhat, targets) # credit assignment loss.backward() # update model weights optimizer.step() . Evaluate the Model . def evaluate_model(test_dl, model): predictions, actuals = list(), list() for i, (inputs, targets) in enumerate(test_dl): # evaluate the model on the test set yhat = model(inputs) # retrieve numpy array yhat = yhat.detach().numpy() actual = targets.numpy() # convert to class labels yhat = argmax(yhat, axis=1) # reshape for stacking actual = actual.reshape((len(actual), 1)) yhat = yhat.reshape((len(yhat), 1)) # store predictions.append(yhat) actuals.append(actual) predictions, actuals = vstack(predictions), vstack(actuals) # calculate accuracy acc = accuracy_score(actuals, predictions) return acc . Execute the above functions (Making Predictions) . path = &#39;./Pytorch Workflow with MNIST&#39; train_dl, test_dl = prepare_data(path) print(len(train_dl.dataset), len(test_dl.dataset)) # define the network model = CNN(1) ## train the model train_model(train_dl, model) # evaluate the model acc = evaluate_model(test_dl, model) print(&#39;Accuracy: %.3f&#39; % acc) ## 60000 10000 ## Accuracy: 0.902 .",
            "url": "https://rakash.github.io/the-data-journo/2020/12/01/Pytorch-Workflow.html",
            "relUrl": "/2020/12/01/Pytorch-Workflow.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Assumptions of Linear regression",
            "content": "One of the most essential steps to take before applying linear regression and depending solely on accuracy scores is to check for these assumptions. . Lets get to it . import numpy as np import pandas as pd import seaborn as sns sns.set(context=&quot;notebook&quot;, palette=&quot;Spectral&quot;, style = &#39;darkgrid&#39; ,font_scale = 1.5, color_codes=True) import warnings warnings.filterwarnings(&#39;ignore&#39;) import os import matplotlib.pyplot as plt . ad_data = pd.read_csv(&#39;/input/Advertising.csv&#39;, index_col=&#39;Unnamed: 0&#39;) . ad_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 200 entries, 1 to 200 Data columns (total 4 columns): TV 200 non-null float64 Radio 200 non-null float64 Newspaper 200 non-null float64 Sales 200 non-null float64 dtypes: float64(4) memory usage: 7.8 KB . ad_data.describe() . TV Radio Newspaper Sales . count 200.000000 | 200.000000 | 200.000000 | 200.000000 | . mean 147.042500 | 23.264000 | 30.554000 | 14.022500 | . std 85.854236 | 14.846809 | 21.778621 | 5.217457 | . min 0.700000 | 0.000000 | 0.300000 | 1.600000 | . 25% 74.375000 | 9.975000 | 12.750000 | 10.375000 | . 50% 149.750000 | 22.900000 | 25.750000 | 12.900000 | . 75% 218.825000 | 36.525000 | 45.100000 | 17.400000 | . max 296.400000 | 49.600000 | 114.000000 | 27.000000 | . p = sns.pairplot(ad_data) . 1. Linearity . Linear regression needs the relationship between the independent and dependent variables to be linear. Let&#39;s use a pair plot to check the relation of independent variables with the Sales variable . p = sns.pairplot(ad_data, x_vars=[&#39;TV&#39;,&#39;Radio&#39;,&#39;Newspaper&#39;], y_vars=&#39;Sales&#39;, size=7, aspect=0.7) . By looking at the plots we can see that with the Sales variable the none of the independent variables form an accurately linear shape but TV and Radio do still better than Newspaper which seems to hardly have any specific shape. So it shows that a linear regression fitting might not be the best model for it. A linear model might not be able to efficiently explain the data in terms of variability, prediction accuracy etc. . A tip is to remember to always see the plots from where the dependent variable is on the y axis. Though it wouldn&#39;t vary the shape much but that&#39;s how linear regression&#39;s intuition is, to put the dependent variable as y and independents as x(s). . Now rest of the assumptions require us to perform the regression before we can even check for them. So let&#39;s perform regression on it. . Fitting the linear model . x = ad_data.drop([&quot;Sales&quot;],axis=1) y = ad_data.Sales . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X = sc.fit_transform(x) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size=0.25) . from sklearn.metrics import mean_absolute_error from sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score from sklearn import linear_model regr = linear_model.LinearRegression() regr.fit(X_train,y_train) y_pred = regr.predict(X_train) . print(&quot;R squared: {}&quot;.format(r2_score(y_true=y_train,y_pred=y_pred))) . R squared: 0.9072183330817297 . 2. Mean of Residuals . Residuals as we know are the differences between the true value and the predicted value. One of the assumptions of linear regression is that the mean of the residuals should be zero. So let&#39;s find out. . residuals = y_train.values-y_pred mean_residuals = np.mean(residuals) print(&quot;Mean of Residuals {}&quot;.format(mean_residuals)) . Mean of Residuals -6.335672727194227e-16 . Very close to zero so all good here. . 3. Check for Homoscedasticity . Homoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms. . Detecting heteroscedasticity! . Graphical Method: Firstly do the regression analysis and then plot the error terms against the predicted values( Yi^). If there is a definite pattern (like linear or quadratic or funnel shaped) obtained from the scatter plot then heteroscedasticity is present. . p = sns.scatterplot(y_pred,residuals) plt.xlabel(&#39;y_pred/predicted values&#39;) plt.ylabel(&#39;Residuals&#39;) plt.ylim(-10,10) plt.xlim(0,26) p = sns.lineplot([0,26],[0,0],color=&#39;blue&#39;) p = plt.title(&#39;Residuals vs fitted values plot for homoscedasticity check&#39;) . Now we will be applying tests. . A tip is to keep in mind that if we want 95% confidence on our findings and tests then the p-value should be less than 0.05 to be able to reject the null hypothesis. Remember, a researcher or data scientist would always aim to reject the null hypothesis. . Goldfeld Quandt Test . Checking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity. . Null Hypothesis: Error terms are homoscedastic | Alternative Hypothesis: Error terms are heteroscedastic. | . import statsmodels.stats.api as sms from statsmodels.compat import lzip from scipy.special import comb name = [&#39;F statistic&#39;, &#39;p-value&#39;] test = sms.het_goldfeldquandt(residuals, X_train) lzip(name, test) . [(&#39;F statistic&#39;, 1.1095600395272505), (&#39;p-value&#39;, 0.33016677253163385)] . Since p value is more than 0.05 in Goldfeld Quandt Test, we can&#39;t reject it&#39;s null hypothesis that error terms are homoscedastic. Good. . Bartlett’s test tests the null hypothesis that all input samples are from populations with equal variances. . from scipy.stats import bartlett test = bartlett( X_train,residuals) . Since p value is quite less than 0.05 in Bartlett, it&#39;s null hypothesis that error terms are homoscedastic gets rejected, that&#39;s not good for a regression. . 4. Check for Normality of error terms/residuals . p = sns.distplot(residuals,kde=True) p = plt.title(&#39;Normality of error terms/residuals&#39;) . The residual terms are pretty much normally distributed for the number of test points we took. Remember the central limit theorem which says that as the sample size increases the distribution tends to be normal. A skew is also visible from the plot. It&#39;s very difficult to get perfect curves, distributions in real life data. . 5. No autocorrelation of residuals . When the residuals are autocorrelated, it means that the current value is dependent of the previous (historic) values and that there is a definite unexplained pattern in the Y variable that shows up in the error terms. Though it is more evident in time series data. . In plain terms autocorrelation takes place when there&#39;s a pattern in the rows of the data. This is usual in time series data as there is a pattern of time for eg. Week of the day effect which is a very famous pattern seen in stock markets where people tend to buy stocks more towards the beginning of weekends and tend to sell more on Mondays. There&#39;s been great study about this phenomenon and it is still a matter of research as to what actual factors cause this trend. . There should not be autocorrelation in the data so the error terms should not form any pattern. . plt.figure(figsize=(10,5)) p = sns.lineplot(y_pred,residuals,marker=&#39;o&#39;,color=&#39;blue&#39;) plt.xlabel(&#39;y_pred/predicted values&#39;) plt.ylabel(&#39;Residuals&#39;) plt.ylim(-10,10) plt.xlim(0,26) p = sns.lineplot([0,26],[0,0],color=&#39;red&#39;) p = plt.title(&#39;Residuals vs fitted values plot for autocorrelation check&#39;) . Checking for autocorrelation To ensure the absence of autocorrelation we use Ljungbox test. . Null Hypothesis: Autocorrelation is absent. | Alternative Hypothesis: Autocorrelation is present. | . from statsmodels.stats import diagnostic as diag min(diag.acorr_ljungbox(residuals , lags = 40)[1]) . 0.008425577339963727 . Since p value is less than 0.05 we reject the null hypothesis that error terms are not autocorrelated. . import statsmodels.api as sm . sm.graphics.tsa.plot_acf(residuals, lags=40) plt.show() . sm.graphics.tsa.plot_pacf(residuals, lags=40) plt.show() . The results show signs of autocorelation since there are spikes outside the red confidence interval region. This could be a factor of seasonality in the data. . 6. No perfect multicollinearity . In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don&#8217;t need to understand the role of each independent variable, you don&#8217;t need to reduce severe multicollinearity. . plt.figure(figsize=(20,20)) # on this line I just set the size of figure to 12 by 10. p=sns.heatmap(ad_data.corr(), annot=True,cmap=&#39;RdYlGn&#39;,square=True) # seaborn has very simple solution for heatmap . Look for correlation of rows where the dependent variable (Sales in this case) is not involved because if a variable is correlated with the dependent variable then this would be a good sign for our model. Correlation within dependent variables is what we need to look for and avoid. This data doesn&#39;t contain perfect multicollinearity among independent variables. In case there was any then we would try to remove one of the correlated variables depending on which was more important to our regression model. . So most of the major assumptions of Linear Regression are successfully through. Great! Since this was one of the simplest data sets it demonstrated the steps well. These steps can be applied on other problems to be able to make better decisions about which model to use. I hope this acts as a decent template of sort to be applied to data. . 7. Some other model evaluations for fun . from sklearn.tree import DecisionTreeRegressor dec_tree = DecisionTreeRegressor(random_state=0) dec_tree.fit(X_train,y_train) dec_tree_y_pred = dec_tree.predict(X_train) print(&quot;Accuracy: {}&quot;.format(dec_tree.score(X_train,y_train))) print(&quot;R squared: {}&quot;.format(r2_score(y_true=y_train,y_pred=dec_tree_y_pred))) . Accuracy: 1.0 R squared: 1.0 . from sklearn.ensemble import RandomForestRegressor rf_tree = RandomForestRegressor(random_state=0) rf_tree.fit(X_train,y_train) rf_tree_y_pred = rf_tree.predict(X_train) print(&quot;Accuracy: {}&quot;.format(rf_tree.score(X_train,y_train))) print(&quot;R squared: {}&quot;.format(r2_score(y_true=y_train,y_pred=rf_tree_y_pred))) . Accuracy: 0.9969092330007508 R squared: 0.9969092330007508 . from sklearn.svm import SVR svr = SVR() svr.fit(X_train,y_train) svr_y_pred = svr.predict(X_train) print(&quot;Accuracy: {}&quot;.format(svr.score(X_train,y_train))) print(&quot;R squared: {}&quot;.format(r2_score(y_true=y_train,y_pred=svr_y_pred))) . Accuracy: 0.9259149689639837 R squared: 0.9259149689639837 . Note that the scores are high because I have used the same data for training and testing. This also shows how significant data splitting, train_test_split() etc. are. This is only for model exploration purposes. Moreover there&#39;s almost no hyperparameter tuning done at this point to make this a simple representation but tuning can highly improve the kind of learning that the model can achieve and keep overfitting away. . Some of the Reference that helped in this blog post . http://r-statistics.co/Assumptions-of-Linear-Regression.html | https://www.statisticssolutions.com/assumptions-of-linear-regression/ | .",
            "url": "https://rakash.github.io/the-data-journo/linear_regression/2020/09/21/LR.html",
            "relUrl": "/linear_regression/2020/09/21/LR.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rakash.github.io/the-data-journo/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rakash.github.io/the-data-journo/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Data Journo is the Blog section of my Website. . I write about data and technology and all posts can be found in the home page. The search tab can help you in searching for a topic thats part of this site. . To know what topics are available, the Tags page should help in searching. . Happy Reading! .",
          "url": "https://rakash.github.io/the-data-journo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rakash.github.io/the-data-journo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}